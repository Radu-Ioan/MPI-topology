processing/process.[cpp|h] -> common attributes for both types of processes
                                (coordinators and workers)
processing/coordinator.[cpp|h] -> coordinator processes files
                                    (0, 1, 2)
processing/worker.[cpp|h] -> worker files

For the stage 1, each coordinator proc reads his cluster from the file,
and the topology is found after the following protocol:

    - in the normal case (with all edges working fine), every coordinator sends
    its cluster to its 2 neighbors, then receives from them the other clusters;

    Aferwards, it is sending toward his workers the known topology, not before
    to print at the output that it knows it. After the workers receive,
    they print it too.

    - for bonus case, 2 is the linking point betwixt 0 and 1. Thinking after
    the chosen protocol, there are 2 steps: first, 0 and 1 send in parallel
    their clusters toward 2, which just waits to receive them (after code
    order, first from 0, then from 1) (it is used
    Coordinator::send_cluster_to_leaders_weak_edge() function).
     
     In the second stage, 2 sends its cluster toward 0 and 1, and these are
     expecting to receive (see
     Coordinator::receive_cluster_from_leaders_weak_edge())

    In both cases (with flawless edge or not), topology spread toward workers
    is executed identically (see Coordinator::send_topology_into_cluster()).

    *Note that for this stage with the bonus varriant (defect on 0-1 edge),
    function names whose ending is 'weak_edge' are not the most suggestive,
    regarding the process 2; However, I preferred to leave as it is, in order
    to observe the symettry between the stages:
    I  0 -> 2 <- 1
    II 0 <- 2 -> 1

For the stage 2:
    0 generates the array, sends data for 1 toward 1, and data for 2 toward 2.
    1 and 2 receive their data, and forward it toward their clusters for being
    processed [dependent_process::forward_stuff_to_workers()]

    Afterwards, still 1 and 2 are waiting for the clusters' results, and after
    they receive all, they begin to transmit them toward 0
        [dependent_process::receive_and_send_results_to_initiator()]

    0 gets the results first from 1, then from 2, then from its cluster.
        [initiator_process::receive_results_from_workers()]
    The last action is to print the obtained array after the calculation.


For the stage 2 without 0-1 edge:
    0 sends data for 1 toward 2;
    0 sends data for 2 toward 2;
        [initiator_process::bad_link::send_stuff_to_leaders()]

    Next, 0 sends the required data for its workers.
        [initiator_process::send_stuff_to_workers()]

    2 receives the calculations for 1 and transmits them toward 1;
    Also, receives data for its cluster and directly sends it to workers.
        [calls dependent_process::bad_link::pivot::forward_data()]

    Afterwards, 1 sends the data got from 2 to its workers
        [calls dependent_process::bad_link::waiting_process
                ::get_data_and_send_it_to_workers()],
    takes back the results from cluster and forwards them toward 2
        [calls dependent_process::bad_link::waiting_process
                ::send_back_results() ]

    2 receives the results from 1 and forwards them to 0;
    After this, receives the calculations made by its cluster and transmits
    them toward 0.

    0 knows the protocol: first, gets from 2 the data processed by 1, and then
    those processed by 2.

    Finally, receives what its workers calculated too. All the got data is
    directly overwritten into the array.


For the second stage, I have no longer put the functions into the 'Coordinator'
structure, as the tasks were not the same for each process.
I preferred to divide into namespaces, especially for bonus case.
For that, I have used the functions with similar purpose:
* initiator_process::start_calculation()
* dependent_process::forward_data()
